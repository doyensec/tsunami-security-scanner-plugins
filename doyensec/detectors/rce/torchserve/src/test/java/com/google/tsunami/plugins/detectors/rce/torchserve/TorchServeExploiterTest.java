package com.google.tsunami.plugins.detectors.rce.torchserve;

import static com.google.common.truth.Truth.assertThat;
import static com.google.tsunami.common.data.NetworkEndpointUtils.forHostnameAndPort;

import org.checkerframework.checker.nullness.qual.Nullable;

import java.io.IOException;
import javax.inject.Inject;

import org.junit.Before;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.junit.runners.JUnit4;

import com.google.tsunami.plugins.detectors.rce.torchserve.TorchServeExploiter;
import com.google.tsunami.plugins.detectors.rce.torchserve.TorchServeManagementApiTestBase;
import com.google.tsunami.proto.*;

import okhttp3.mockwebserver.MockResponse;

@RunWith(JUnit4.class)
public final class TorchServeExploiterTest extends TorchServeManagementApiTestBase {
    @Inject
    private TorchServeExploiter exploiter;

    NetworkService service;

    @Before
    public void setUpNetworkService() {
        service = NetworkService.newBuilder()
            .setNetworkEndpoint(forHostnameAndPort(mockTorchServe.getHostName(), mockTorchServe.getPort()))
            .setTransportProtocol(TransportProtocol.TCP)
            .setSoftware(Software.newBuilder().setName("torchserve"))
            .setServiceName("http")
            .build();
    }

    private void enqueueMockTorchServeResponse(String response) {
        mockTorchServe.enqueue(
            new MockResponse()
                .setResponseCode(200)
                .setBody(response)
        );
    }

    private String API_DESCRIPTION_RESPONSE = "{\n"
        + "  \"openapi\": \"3.0.1\",\n"
        + "  \"info\": {\n"
        + "    \"title\": \"TorchServe APIs\",\n"
        + "    \"description\": \"TorchServe is a flexible and easy to use tool for serving deep learning models\",\n"
        + "    \"version\": \"0.8.1\"\n"
        + "  },\n"
        + "  \"paths\": {\n"
        + "    \"/models\": {\n"
        + "      \"post\": {\n"
        + "        \"description\": \"Register a new model in TorchServe.\",\n"
        + "        \"operationId\": \"registerModel\"\n"
        + "     }\n"
        + "    }\n"
        + "  }\n"
        + "}";

    private String EMPTY_MODELS_RESPONSE = "{\"models\": []}";

    private String getCustomizedMetadataResponse(@Nullable String metadata) {
        return "[{ \"customizedMetadata\" : \"" +
            (metadata == null ? "" : metadata) +
            "\" }]";
    }

    @Test
    public void isServiceVulnerable_returnsNullIfServiceIsNotTorchServe() throws IOException {
        // This is template of Inference API response not Management API (no POST /models)
        enqueueMockTorchServeResponse(
            "{\n"
                + "  \"openapi\": \"3.0.1\",\n"
                + "  \"info\": {\n"
                + "    \"title\": \"TorchServe APIs\",\n"
                + "    \"description\": \"TorchServe is a flexible and easy to use tool for serving deep learning models\",\n"
                + "    \"version\": \"0.8.1\"\n"
                + "  },\n"
                + "  \"paths\": {\n"
                + "    \"/metrics\": {\n"
                + "      \"get\": {\n"
                + "        \"description\": \"Get TorchServe application metrics in prometheus format.\",\n"
                + "        \"operationId\": \"metrics\"\n"
                + "     }\n"
                + "    }\n"
                + "  }\n"
                + "}"
        );
        assertThat(exploiter.isServiceVulnerable(service)).isNull();
    }

    @Test
    public void isServiceVulnerable_returnsDetailsIfServiceIsVulnerableBasic() throws IOException {
        enqueueMockTorchServeResponse(API_DESCRIPTION_RESPONSE);
        // Generate the JSON response with the array of models:
        // "models": [
        //   {
        //     "modelName": "squeezenet1_1",
        //     "modelUrl": "https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar"
        //   },
        mockTorchServe.enqueue(
            new MockResponse()
                .setResponseCode(200)
                .setBody(
                    "{\"models\": [\n"
                        + "{\n"
                        + "  \"status\": \"SUCCESS\",\n"
                        + "  \"modelName\": \"squeezenet1_1\",\n"
                        + "  \"modelUrl\": \"https://torchserve.pytorch.org/mar_files/squeezenet1_1.mar\"\n"
                        + "}]}"
                )
        );

        TorchServeExploiter.Details details = exploiter.isServiceVulnerable(service);

        assertThat(details).isNotNull();
        assertThat(details.models).containsExactly("squeezenet1_1");
        assertThat(details.getSeverity()).isEqualTo(Severity.LOW);
        assertThat(details.isVerified()).isFalse();
        assertThat(details.generateDescription()).isEqualTo(
            "An exposed TorchServe management API was detected on the target. TorchServe is a model server for PyTorch models. The management API allows adding new models to the server which by design can be used to execute arbitrary code on the target.\nThis exposure poses a significant security risk as it could allow unauthorized users to run arbitrary code on the server."
        );
        assertThat(details.generateAdditionalDetails()).isEqualTo(
            "Callback verification is not enabled in Tsunami configuration, so the exploit could not be confirmed and only the Management API detection is reported. It is recommended to enable callback verification for more conclusive vulnerability assessment.\nModels found on the target:\n  - squeezenet1_1"
        );
    }

    @Test
    public void isServiceVulnerable_successfulExploitInStaticMode() throws IOException {
        // Setup the details for STATIC mode
        exploiter.details.exploitationMode = TorchServeExploiter.ExploitationMode.STATIC;
        exploiter.details.staticUrl = "http://mock-static-url.com/model.mar";

        enqueueMockTorchServeResponse(API_DESCRIPTION_RESPONSE);

        // Mocking the response for listing models - assuming an empty list for simplicity
        enqueueMockTorchServeResponse(EMPTY_MODELS_RESPONSE);

        // Mocking the response for removeModelByUrl
        enqueueMockTorchServeResponse(EMPTY_MODELS_RESPONSE);

        // Mocking the response for model registration
        mockTorchServe.enqueue(new MockResponse()
            .setResponseCode(200)
            .setBody(
                "{\n"
                    + "  \"status\": \"Model \\\"squeezenet1_1\\\" Version: 1.0 registered with 1 initial workers\"\n"
                    + "}"
            )
        );

        // Mocking the response for model list to confirm the model was registered
        enqueueMockTorchServeResponse("");

        // Mocking the response to hash verification request
        enqueueMockTorchServeResponse(getCustomizedMetadataResponse(null));

        // Mocking the response to adding a log file
        enqueueMockTorchServeResponse(getCustomizedMetadataResponse(null));

        // Mocking the response to system info request
        enqueueMockTorchServeResponse(getCustomizedMetadataResponse("{}"));

        // Perform the exploitation test
        TorchServeExploiter.Details details = exploiter.isServiceVulnerable(service);

        // Assertions
        assertThat(details).isNotNull();
        assertThat(details.exploitationMode).isEqualTo(TorchServeExploiter.ExploitationMode.STATIC);
        assertThat(details.exploitUrl).isEqualTo(exploiter.details.staticUrl);
        assertThat(details.isVerified()).isTrue();
    }
}
